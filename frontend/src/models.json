{
  "predefined_models": [
    {
      "name": "Qwen/Qwen2.5-1.5B-Instruct-AWQ",
      "abbr": "qwen1.5b-awq",
      "type": "llm",
      "description": "Qwen 2.5 1.5B (4-bit AWQ) - Efficient",
      "quantization": "awq",
      "max_model_len": 2048,
      "recommended_settings": {
        "gpu_memory_utilization": 0.25,
        "max_num_seqs": 256
      }
    },
    {
      "name": "Qwen/Qwen2.5-7B-Instruct-AWQ",
      "abbr": "qwen7b-awq",
      "type": "llm",
      "description": "Qwen 2.5 7B (4-bit AWQ) - Efficient",
      "quantization": "awq",
      "max_model_len": 2048,
      "recommended_settings": {
        "gpu_memory_utilization": 0.25,
        "max_num_seqs": 256
      }
    },
    {
      "name": "TheBloke/Mistral-7B-Instruct-v0.2-AWQ",
      "abbr": "mistral7b-awq",
      "type": "llm",
      "description": "Mistral 7B v0.2 (4-bit AWQ) - Fast inference",
      "quantization": "awq",
      "max_model_len": 2048,
      "recommended_settings": {
        "gpu_memory_utilization": 0.25,
        "max_num_seqs": 256
      }
    },
    {
      "name": "TheBloke/Llama-2-13B-chat-AWQ",
      "abbr": "llama13b-awq",
      "type": "llm",
      "description": "Llama 2 13B Chat (4-bit AWQ)",
      "quantization": "awq",
      "max_model_len": 2048,
      "recommended_settings": {
        "gpu_memory_utilization": 0.25,
        "max_num_seqs": 256
      }
    },
    {
      "name": "BAAI/bge-large-en-v1.5",
      "abbr": "bge-large",
      "type": "embedding",
      "description": "BGE Large English embedding model",
      "max_model_len": 512,
      "recommended_settings": {
        "gpu_memory_utilization": 0.05,
        "max_num_seqs": 512
      }
    },
    {
      "name": "BAAI/bge-base-en-v1.5",
      "abbr": "bge-base",
      "type": "embedding",
      "description": "BGE Base English embedding model",
      "max_model_len": 512,
      "recommended_settings": {
        "gpu_memory_utilization": 0.05,
        "max_num_seqs": 1024
      }
    },
    {
      "name": "sentence-transformers/all-MiniLM-L6-v2",
      "abbr": "minilm",
      "type": "embedding",
      "description": "Lightweight embedding model",
      "max_model_len": 256,
      "recommended_settings": {
        "gpu_memory_utilization": 0.05,
        "max_num_seqs": 1024
      }
    },
    {
      "name": "BM-K/KoSimCSE-roberta",
      "abbr": "kosimcse-roberta",
      "type": "embedding",
      "description": "Korean-Sentence-Embedding",
      "max_model_len": 512,
      "recommended_settings": {
        "gpu_memory_utilization": 0.05,
        "max_num_seqs": 1024
      }
    },
    {
      "name": "snunlp/KR-SBERT-V40K-klueNLI-augSTS",
      "abbr": "kr-sbert",
      "type": "embedding",
      "description": "Korean sentence-transformers model",
      "max_model_len": 128,
      "recommended_settings": {
        "gpu_memory_utilization": 0.05,
        "max_num_seqs": 1024
      }
    }
  ],
  "gpu_allocation_strategy": {
    "A6000": {
      "total_vram_gb": 48,
      "suggested_combinations": [
        {
          "name": "Balanced LLM + Embeddings",
          "models": [
            {"abbr": "llama13b-awq", "vram_gb": 8},
            {"abbr": "qwen7b", "vram_gb": 14},
            {"abbr": "bge-large", "vram_gb": 2},
            {"abbr": "bge-base", "vram_gb": 1}
          ],
          "total_vram_gb": 25,
          "remaining_gb": 23
        },
        {
          "name": "Maximum LLMs",
          "models": [
            {"abbr": "llama8b", "vram_gb": 16},
            {"abbr": "mistral7b", "vram_gb": 14},
            {"abbr": "qwen7b", "vram_gb": 14}
          ],
          "total_vram_gb": 44,
          "remaining_gb": 4
        },
        {
          "name": "RAG Optimized",
          "models": [
            {"abbr": "llama13b-awq", "vram_gb": 8},
            {"abbr": "bge-large", "vram_gb": 2},
            {"abbr": "reranker-m3", "vram_gb": 3},
            {"abbr": "minilm", "vram_gb": 1}
          ],
          "total_vram_gb": 14,
          "remaining_gb": 34
        }
      ]
    }
  }
}