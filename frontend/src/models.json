{
  "predefined_models": [
    {
      "name": "Qwen/Qwen2.5-1.5B-Instruct",
      "abbr": "qwen1.5b",
      "type": "llm",
      "description": "Qwen 2.5 1.5B - Ultra lightweight model",
      "max_model_len": 32768,
      "recommended_settings": {
        "gpu_memory_utilization": 0.3,
        "max_num_seqs": 512
      }
    },
    {
      "name": "Qwen/Qwen2.5-3B-Instruct",
      "abbr": "qwen3b",
      "type": "llm",
      "description": "Qwen 2.5 3B - Small but capable model",
      "max_model_len": 32768,
      "recommended_settings": {
        "gpu_memory_utilization": 0.5,
        "max_num_seqs": 384
      }
    },
    {
      "name": "microsoft/phi-2",
      "abbr": "phi2",
      "type": "llm",
      "description": "Phi-2 2.7B - Efficient small model",
      "max_model_len": 2048,
      "recommended_settings": {
        "gpu_memory_utilization": 0.4,
        "max_num_seqs": 512
      }
    },
    {
      "name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      "abbr": "tinyllama",
      "type": "llm",
      "description": "TinyLlama 1.1B - Extremely lightweight",
      "max_model_len": 2048,
      "recommended_settings": {
        "gpu_memory_utilization": 0.2,
        "max_num_seqs": 1024
      }
    },
    {
      "name": "meta-llama/Llama-3.1-8B-Instruct",
      "abbr": "llama8b",
      "type": "llm",
      "description": "Llama 3.1 8B instruction-tuned model",
      "max_model_len": 8192,
      "recommended_settings": {
        "gpu_memory_utilization": 0.9,
        "max_num_seqs": 256
      }
    },
    {
      "name": "Qwen/Qwen2.5-1.5B-Instruct-AWQ",
      "abbr": "qwen1.5b-awq",
      "type": "llm",
      "description": "Qwen 2.5 1.5B (4-bit AWQ) - Ultra fast",
      "quantization": "awq",
      "max_model_len": 32768,
      "recommended_settings": {
        "gpu_memory_utilization": 0.15,
        "max_num_seqs": 1024
      }
    },
    {
      "name": "Qwen/Qwen2.5-3B-Instruct-AWQ",
      "abbr": "qwen3b-awq",
      "type": "llm",
      "description": "Qwen 2.5 3B (4-bit AWQ) - Fast & capable",
      "quantization": "awq",
      "max_model_len": 32768,
      "recommended_settings": {
        "gpu_memory_utilization": 0.25,
        "max_num_seqs": 768
      }
    },
    {
      "name": "Qwen/Qwen2.5-7B-Instruct-AWQ",
      "abbr": "qwen7b-awq",
      "type": "llm",
      "description": "Qwen 2.5 7B (4-bit AWQ) - Efficient",
      "quantization": "awq",
      "max_model_len": 32768,
      "recommended_settings": {
        "gpu_memory_utilization": 0.45,
        "max_num_seqs": 512
      }
    },
    {
      "name": "TheBloke/Llama-2-13B-chat-AWQ",
      "abbr": "llama13b-awq",
      "type": "llm",
      "description": "Llama 2 13B Chat (4-bit AWQ)",
      "quantization": "awq",
      "max_model_len": 4096,
      "recommended_settings": {
        "gpu_memory_utilization": 0.85,
        "max_num_seqs": 256
      }
    },
    {
      "name": "TheBloke/Mistral-7B-Instruct-v0.2-AWQ",
      "abbr": "mistral7b-awq",
      "type": "llm",
      "description": "Mistral 7B v0.2 (4-bit AWQ) - Fast inference",
      "quantization": "awq",
      "max_model_len": 32768,
      "recommended_settings": {
        "gpu_memory_utilization": 0.45,
        "max_num_seqs": 512
      }
    },
    {
      "name": "TheBloke/Llama-2-7B-Chat-GPTQ",
      "abbr": "llama7b-gptq",
      "type": "llm",
      "description": "Llama 2 7B Chat (4-bit GPTQ)",
      "quantization": "gptq",
      "max_model_len": 4096,
      "recommended_settings": {
        "gpu_memory_utilization": 0.45,
        "max_num_seqs": 384
      }
    },
    {
      "name": "Qwen/Qwen2.5-7B-Instruct",
      "abbr": "qwen7b",
      "type": "llm",
      "description": "Qwen 2.5 7B instruction model",
      "max_model_len": 32768,
      "recommended_settings": {
        "gpu_memory_utilization": 0.9,
        "max_num_seqs": 256
      }
    },
    {
      "name": "mistralai/Mistral-7B-Instruct-v0.3",
      "abbr": "mistral7b",
      "type": "llm",
      "description": "Mistral 7B v0.3 instruction model",
      "max_model_len": 32768,
      "recommended_settings": {
        "gpu_memory_utilization": 0.9,
        "max_num_seqs": 256
      }
    },
    {
      "name": "BAAI/bge-large-en-v1.5",
      "abbr": "bge-large",
      "type": "embedding",
      "description": "BGE Large English embedding model",
      "max_model_len": 512,
      "recommended_settings": {
        "gpu_memory_utilization": 0.3,
        "max_num_seqs": 512
      }
    },
    {
      "name": "BAAI/bge-base-en-v1.5",
      "abbr": "bge-base",
      "type": "embedding",
      "description": "BGE Base English embedding model",
      "max_model_len": 512,
      "recommended_settings": {
        "gpu_memory_utilization": 0.2,
        "max_num_seqs": 1024
      }
    },
    {
      "name": "sentence-transformers/all-MiniLM-L6-v2",
      "abbr": "minilm",
      "type": "embedding",
      "description": "Lightweight embedding model",
      "max_model_len": 256,
      "recommended_settings": {
        "gpu_memory_utilization": 0.1,
        "max_num_seqs": 2048
      }
    },
    {
      "name": "BAAI/bge-reranker-v2-m3",
      "abbr": "reranker-m3",
      "type": "reranker",
      "description": "BGE Reranker v2 M3",
      "max_model_len": 8192,
      "recommended_settings": {
        "gpu_memory_utilization": 0.2,
        "max_num_seqs": 256
      }
    },
    {
      "name": "microsoft/Phi-3.5-vision-instruct",
      "abbr": "phi-vision",
      "type": "vision",
      "description": "Phi 3.5 Vision model",
      "max_model_len": 4096,
      "recommended_settings": {
        "gpu_memory_utilization": 0.8,
        "max_num_seqs": 32
      }
    }
  ],
  "gpu_allocation_strategy": {
    "A6000": {
      "total_vram_gb": 48,
      "suggested_combinations": [
        {
          "name": "Balanced LLM + Embeddings",
          "models": [
            {"abbr": "llama13b-awq", "vram_gb": 8},
            {"abbr": "qwen7b", "vram_gb": 14},
            {"abbr": "bge-large", "vram_gb": 2},
            {"abbr": "bge-base", "vram_gb": 1}
          ],
          "total_vram_gb": 25,
          "remaining_gb": 23
        },
        {
          "name": "Maximum LLMs",
          "models": [
            {"abbr": "llama8b", "vram_gb": 16},
            {"abbr": "mistral7b", "vram_gb": 14},
            {"abbr": "qwen7b", "vram_gb": 14}
          ],
          "total_vram_gb": 44,
          "remaining_gb": 4
        },
        {
          "name": "RAG Optimized",
          "models": [
            {"abbr": "llama13b-awq", "vram_gb": 8},
            {"abbr": "bge-large", "vram_gb": 2},
            {"abbr": "reranker-m3", "vram_gb": 3},
            {"abbr": "minilm", "vram_gb": 1}
          ],
          "total_vram_gb": 14,
          "remaining_gb": 34
        }
      ]
    }
  }
}